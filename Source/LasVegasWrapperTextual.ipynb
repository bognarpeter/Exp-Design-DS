{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training data\n",
    "textual_df = pd.read_csv(\"../Results/textual.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "metadata_x = textual_df.drop(\"goodforairplane\", axis=1)\n",
    "# Target\n",
    "metadata_Y = textual_df[\"goodforairplane\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(50)\n",
    "\n",
    "def LVW(meta_x, meta_Y, K, classifier):\n",
    "    err = 0\n",
    "    k = 0\n",
    "    C = 100\n",
    "    # Best features\n",
    "    S = np.array([])\n",
    "    # Create list of features\n",
    "    while k < K:\n",
    "        # Create a list of available features\n",
    "        features_ind = np.array(list(range(0, len(meta_x.columns))))\n",
    "        num_features = len(features_ind)\n",
    "        # Generate how many features should be selected\n",
    "        C1 = np.random.randint(1,num_features+1)\n",
    "        # Randomly pick num_selected_features as subset\n",
    "        S1_ind = np.random.choice(features_ind, size=C1, replace = False)\n",
    "        # Sort it, because it looks nicer...\n",
    "        S1_ind = np.sort(S1_ind)\n",
    "        # Train the classifier\n",
    "        classifier.fit(meta_x.iloc[:,S1_ind], meta_Y)\n",
    "        # Predict the results\n",
    "        pred = classifier.predict(meta_x.iloc[:,S1_ind])\n",
    "        # Comptue f1 measure (since in the paper it is stated that they tune it based on the f1 score)\n",
    "        f1 = f1_score(meta_Y, pred)\n",
    "        \n",
    "        if (f1 > err or (f1 == err and C1 < C)):\n",
    "            k = 0\n",
    "            S = S1_ind\n",
    "            err = f1\n",
    "            C = C1\n",
    "        else:\n",
    "            k = k+1\n",
    "    # Test on testset\n",
    "    classifier.fit(meta_x.iloc[:,S], meta_Y)\n",
    "    #test_pred = classifier.predict()\n",
    "    \n",
    "    # Get column names\n",
    "    columns = meta_x.iloc[:,S].columns\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: KNeighborsClassifier, Modality: metadata, Precision: 0.580, Recall: 0.603, F1: 0.566\n",
      "Classifier: SVC, Modality: metadata, Precision: 0.548, Recall: 1.000, F1: 0.708\n",
      "Classifier: GaussianNB, Modality: metadata, Precision: 0.554, Recall: 0.593, F1: 0.556\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(50)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define Scoring methods\n",
    "scoring = {'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "classifiers = [KNeighborsClassifier(),\n",
    "               SVC(), GaussianNB()]\n",
    "\n",
    "for classi in classifiers:\n",
    "    # Perform LVW (not for random forest)\n",
    "    if isinstance(classi, RandomForestClassifier):\n",
    "        ind = LVW(metadata_x, metadata_Y, 10, classi)\n",
    "        scores = cross_validate(classi, X=metadata_x, y=metadata_Y, cv=10,scoring=scoring)\n",
    "        f_1 = np.mean(scores[\"test_f1_score\"])\n",
    "        precision = np.mean(scores[\"test_precision\"])\n",
    "        recall = np.mean(scores[\"test_recall\"])\n",
    "    else:\n",
    "        ind = LVW(metadata_x, metadata_Y, 10, classi)\n",
    "        scores = cross_validate(classi, X=metadata_x[ind], y=metadata_Y, cv=10,scoring=scoring)\n",
    "        f_1 = np.mean(scores[\"test_f1_score\"])\n",
    "        precision = np.mean(scores[\"test_precision\"])\n",
    "        recall = np.mean(scores[\"test_recall\"])\n",
    "    print(\"Classifier: %s, Modality: metadata, Precision: %.3f, Recall: %.3f, F1: %.3f\" % (classi.__class__.__name__, precision, recall, f_1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
